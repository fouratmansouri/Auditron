{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11387766,"sourceType":"datasetVersion","datasetId":7131097}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytesseract pdf2image opencv-python-headless numpy langdetect pymupdf pandas sentence-transformers faiss-cpu flask tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:51:03.068747Z","iopub.execute_input":"2025-04-13T11:51:03.069160Z","iopub.status.idle":"2025-04-13T11:51:15.781583Z","shell.execute_reply.started":"2025-04-13T11:51:03.069120Z","shell.execute_reply":"2025-04-13T11:51:15.780209Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Downloading the documents","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport requests\nfrom urllib.parse import urlparse\nimport pytesseract\nfrom pdf2image import convert_from_path\nimport cv2\nimport numpy as np\nfrom langdetect import detect\nimport fitz  # PyMuPDF\nimport re\nimport logging\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"ocr_processing.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# Load the Excel file and download files\ndef download_files_by_type(excel_path, download_base_dir=\"downloads\", verify_ssl=False):\n    df = pd.read_excel(excel_path)\n    if not os.path.exists(download_base_dir):\n        os.makedirs(download_base_dir)\n\n    for index, row in df.iterrows():\n        file_name = row['titre']\n        file_url = row['url']\n        file_type = row['type']\n        \n        type_dir = os.path.join(download_base_dir, file_type)\n        os.makedirs(type_dir, exist_ok=True)\n\n        # Derive file extension from URL if possible\n        parsed_url = urlparse(file_url)\n        url_path = parsed_url.path\n        ext_from_url = os.path.splitext(url_path)[1].lower()  # e.g., \".pdf\"\n\n        # Clean filename or fallback to default\n        if pd.isna(file_name) or file_name.strip() == \"\":\n            file_name = os.path.basename(url_path)\n            if not file_name:\n                file_name = f\"file_{index}{ext_from_url if ext_from_url else '.bin'}\"\n        else:\n            # Ensure file_name has proper extension\n            if ext_from_url and not file_name.lower().endswith(ext_from_url):\n                file_name += ext_from_url\n\n        download_path = os.path.join(type_dir, file_name)\n        print(f\"Downloading {file_url} to {download_path}\")\n\n        try:\n            response = requests.get(file_url, stream=True, verify=verify_ssl)\n            response.raise_for_status()\n            with open(download_path, 'wb') as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            print(f\"Successfully downloaded {file_name}\")\n        except Exception as e:\n            print(f\"Error downloading {file_url}: {e}\")\n\n# Document Processor class (same as your original, kept unchanged)\nclass DocumentProcessor:\n    def __init__(self, excel_path, download_base_dir=\"/kaggle/working/downloads\", output_dir=\"processed_texts\"):\n        self.excel_path = excel_path\n        self.download_base_dir = download_base_dir\n        self.output_dir = output_dir\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        self.df = pd.read_excel(excel_path)\n        # pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n\n    def process_all_documents(self):\n        results = []\n        arabic_txt_source_dir = \"/kaggle/input/legal-documents/legal_text/parsed_text\"\n    \n        for index, row in self.df.iterrows():\n            try:\n                file_name = row['titre']\n                file_type = row['type']\n                language = row.get('langue', 'fr')\n    \n                if pd.isna(file_name) or file_name == \"\":\n                    file_name = f\"file_{index}.bin\"\n    \n                if not file_name.lower().endswith('.pdf'):\n                    file_name += '.pdf'\n    \n                base_name = os.path.splitext(file_name)[0]\n                text_file_name = base_name + \".txt\"\n                text_file_path = os.path.join(self.output_dir, text_file_name)\n    \n                if language == 'ar':\n                    # For Arabic, just copy the existing parsed text file\n                    source_txt_path = os.path.join(arabic_txt_source_dir, text_file_name)\n                    if os.path.exists(source_txt_path):\n                        with open(source_txt_path, 'r', encoding='utf-8') as src_file:\n                            content = src_file.read()\n                        with open(text_file_path, 'w', encoding='utf-8') as dst_file:\n                            dst_file.write(content)\n                        logger.info(f\"Copied parsed Arabic text for {file_name}\")\n                        results.append({\n                            'file_name': file_name,\n                            'language': language,\n                            'text_file': text_file_path,\n                            'status': 'copied from parsed_text'\n                        })\n                    else:\n                        logger.warning(f\"Parsed Arabic file not found: {source_txt_path}\")\n                        results.append({\n                            'file_name': file_name,\n                            'language': language,\n                            'text_file': None,\n                            'status': 'missing parsed Arabic text'\n                        })\n                    continue  # Skip further processing for Arabic\n                # For French or others, process as usual\n                file_path = os.path.join(self.download_base_dir, file_type, file_name)\n                if not os.path.exists(file_path):\n                    logger.warning(f\"File not found: {file_path}\")\n                    continue\n\n                if file_path.lower().endswith('.pdf'):\n                    text = self.process_pdf(file_path, language)\n                    with open(text_file_path, 'w', encoding='utf-8') as f:\n                        f.write(text)\n                    results.append({\n                        'file_name': file_name,\n                        'language': language,\n                        'text_file': text_file_path,\n                        'status': 'processed'\n                    })\n                    logger.info(f\"Successfully processed {file_name}\")\n                else:\n                    logger.warning(f\"Unsupported file format: {file_path}\")\n    \n            except Exception as e:\n                logger.error(f\"Error processing document {file_name if 'file_name' in locals() else index}: {str(e)}\")\n                results.append({\n                    'file_name': file_name if 'file_name' in locals() else f\"document_{index}\",\n                    'status': 'error',\n                    'error_message': str(e)\n                })\n    \n        results_df = pd.DataFrame(results)\n        results_df.to_csv(os.path.join(self.output_dir, \"processing_results.csv\"), index=False)\n        return results_df\n\n    def process_pdf(self, pdf_path, language):\n        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n        \n        if language == 'ar':\n            # Skip processing and load already translated text from the provided folder\n            translated_path = f\"/kaggle/input/legal-documents/legal_text/parsed_text/{base_name}.txt\"\n            if os.path.exists(translated_path):\n                logger.info(f\"Loading Arabic translation for {pdf_path} from {translated_path}\")\n                try:\n                    with open(translated_path, 'r', encoding='utf-8') as f:\n                        return f.read()\n                except Exception as e:\n                    logger.error(f\"Error reading translated Arabic file: {e}\")\n                    return \"\"\n            else:\n                logger.warning(f\"Translation not found for Arabic file: {base_name}\")\n                return \"\"\n    \n        # For French documents, proceed as normal\n        extracted_text = self.extract_text_from_pdf(pdf_path)\n        if not extracted_text or len(extracted_text.strip()) < 100:\n            logger.info(f\"Applying OCR to {pdf_path} (language: {language})\")\n            extracted_text = self.apply_ocr(pdf_path, language)\n        else:\n            logger.info(f\"Text successfully extracted directly from {pdf_path}\")\n        return extracted_text\n\n\n    def extract_text_from_pdf(self, pdf_path):\n        try:\n            text = \"\"\n            pdf_document = fitz.open(pdf_path)\n            for page_num in range(len(pdf_document)):\n                page = pdf_document[page_num]\n                text += page.get_text()\n            pdf_document.close()\n            return text\n        except Exception as e:\n            logger.error(f\"Error extracting text directly from PDF: {str(e)}\")\n            return \"\"\n\n    def apply_ocr(self, pdf_path, language):\n        tesseract_lang = 'ara' if language == 'ar' else 'fra'\n        try:\n            images = convert_from_path(pdf_path)\n        except Exception as e:\n            logger.error(f\"Error converting PDF to images: {str(e)}\")\n            return \"\"\n        full_text = \"\"\n        for i, image in enumerate(images):\n            logger.info(f\"Processing page {i+1}/{len(images)} of {pdf_path}\")\n            open_cv_image = np.array(image)[:, :, ::-1].copy()\n            if language == 'ar':\n                preprocessed = self.preprocess_image_arabic(open_cv_image)\n            else:\n                preprocessed = self.preprocess_image_french(open_cv_image)\n            try:\n                config = '--psm 6 --oem 1' if language == 'ar' else ''\n                text = pytesseract.image_to_string(preprocessed, lang=tesseract_lang, config=config)\n                full_text += text + \"\\n\\n\"\n            except Exception as e:\n                logger.error(f\"OCR error on page {i+1}: {str(e)}\")\n        full_text = self.clean_text(full_text, language)\n        return full_text\n\n    def preprocess_image_arabic(self, image):\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n        kernel = np.ones((1, 1), np.uint8)\n        dilated = cv2.dilate(binary, kernel, iterations=1)\n        return dilated\n\n    def preprocess_image_french(self, image):\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        binary = cv2.adaptiveThreshold(\n            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2\n        )\n        denoised = cv2.fastNlMeansDenoising(binary, None, 10, 7, 21)\n        return denoised\n\n    def clean_text(self, text, language):\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'\\s[^\\w\\s]\\s', ' ', text)\n        text = text.replace('|', 'I')\n        return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:51:38.039131Z","iopub.execute_input":"2025-04-13T11:51:38.039502Z","iopub.status.idle":"2025-04-13T11:51:38.231382Z","shell.execute_reply.started":"2025-04-13T11:51:38.039471Z","shell.execute_reply":"2025-04-13T11:51:38.230436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    excel_file = \"/kaggle/input/legal-documents/legal_text/documents_fiscaux.xlsx\"\n    download_dir = \"/kaggle/working/downloads\"\n    output_dir = \"/kaggle/working/processed_texts\"\n    \n    # Download all files\n    download_files_by_type(excel_file, download_base_dir=download_dir, verify_ssl=False)\n    \n    # Check if files were downloaded\n    print(\"Checking downloaded files:\")\n    file_count = 0\n    for root, dirs, files in os.walk(download_dir):\n        for file in files:\n            file_count += 1\n            print(f\"Found file: {os.path.join(root, file)}\")\n    \n    if file_count == 0:\n        print(\"No files were downloaded. Check download errors.\")\n        exit(1)\n    \n    # Debug Excel file reading\n    print(\"\\nAttempting to read Excel file:\")\n    try:\n        df = pd.read_excel(excel_file)\n        print(f\"Excel file read successfully. Shape: {df.shape}\")\n        print(f\"Columns: {df.columns.tolist()}\")\n        print(f\"First 3 rows:\")\n        print(df.head(3))\n        \n        # Check if required columns exist\n        required_columns = ['titre', 'type', 'url']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            print(f\"Warning: Missing required columns: {missing_columns}\")\n        \n        # Debug file path construction\n        print(\"\\nAttempting to match downloaded files with Excel entries:\")\n        for index, row in df.iterrows():\n            if index > 5:  # Limit to first few rows for debugging\n                break\n                \n            print(f\"Row {index}:\")\n            file_name = row.get('titre', None)\n            file_type = row.get('type', None)\n            \n            print(f\"  titre: {file_name}\")\n            print(f\"  type: {file_type}\")\n            \n            if pd.isna(file_name) or file_name == \"\":\n                file_name = f\"file_{index}.bin\"\n                \n            expected_path = os.path.join(download_dir, file_type, file_name)\n            print(f\"  Looking for file: {expected_path}\")\n            print(f\"  File exists: {os.path.exists(expected_path)}\")\n    except Exception as e:\n        print(f\"Error reading Excel file: {str(e)}\")\n    \n    # Run processing\n    try:\n        processor = DocumentProcessor(\n            excel_path=excel_file,\n            download_base_dir=download_dir,\n            output_dir=output_dir\n        )\n        results = processor.process_all_documents()\n        print(f\"Processing completed with {len(results)} files. Results saved to {output_dir}/processing_results.csv\")\n    except Exception as e:\n        print(f\"Error during document processing: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T11:51:40.231417Z","iopub.execute_input":"2025-04-13T11:51:40.231876Z","iopub.status.idle":"2025-04-13T11:53:04.122586Z","shell.execute_reply.started":"2025-04-13T11:51:40.231845Z","shell.execute_reply":"2025-04-13T11:53:04.121573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport json\n\ndef generate_metadata_json(xlsx_path, processed_texts_dir, output_json_path):\n    # Load metadata Excel\n    df = pd.read_excel(xlsx_path)\n\n    all_entries = []\n\n    for _, row in df.iterrows():\n        entry = row.to_dict()\n        file_name = row.get('titre')\n\n        if pd.isna(file_name) or file_name == \"\":\n            file_name = f\"file_{_}\"\n\n        if not file_name.lower().endswith('.pdf'):\n            file_name += '.pdf'\n\n        base_name = os.path.splitext(file_name)[0]\n        txt_file_name = base_name + \".txt\"\n        txt_file_path = os.path.join(processed_texts_dir, txt_file_name)\n\n        try:\n            with open(txt_file_path, 'r', encoding='utf-8') as f:\n                entry['text'] = f.read()\n        except FileNotFoundError:\n            entry['text'] = None\n            print(f\"Warning: Text file not found for {file_name}\")\n\n        all_entries.append(entry)\n\n    # Save to JSON\n    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n        json.dump(all_entries, json_file, ensure_ascii=False, indent=2)\n\n    print(f\"✅ JSON file saved to: {output_json_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T12:00:40.530681Z","iopub.execute_input":"2025-04-13T12:00:40.531056Z","iopub.status.idle":"2025-04-13T12:00:40.538695Z","shell.execute_reply.started":"2025-04-13T12:00:40.531029Z","shell.execute_reply":"2025-04-13T12:00:40.537507Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"generate_metadata_json(\n    xlsx_path=\"/kaggle/input/legal-documents/legal_text/documents_fiscaux.xlsx\",\n    processed_texts_dir=\"/kaggle/working/processed_texts\",\n    output_json_path=\"/kaggle/working/legal_documents_dataset.json\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T12:01:13.283627Z","iopub.execute_input":"2025-04-13T12:01:13.283960Z","iopub.status.idle":"2025-04-13T12:01:13.370853Z","shell.execute_reply.started":"2025-04-13T12:01:13.283934Z","shell.execute_reply":"2025-04-13T12:01:13.369629Z"}},"outputs":[{"name":"stdout","text":"✅ JSON file saved to: /kaggle/working/legal_documents_dataset.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}