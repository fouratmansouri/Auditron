{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_chroma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaLLM as Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm():\n",
    "    return Ollama(model=\"deepseek-r1:32b\", temperature=0.1)\n",
    "\n",
    "def init_routing_llm():\n",
    "    return Ollama(model=\"mistral\", temperature=0.1, top_p=0.9, top_k=40)\n",
    "\n",
    "def classify_question(routing_llm, question):\n",
    "    prompt = \"\"\"Votre mission est de classifier la question suivante en fonction de sa complexité. Elle est posée par un expert fiscal tunisien. \n",
    "    Vous ne connaissez rien sur la fiscalité tunisienne sauf des généralités (définitions de TVA, retenue à la source, etc.). \n",
    "    Si la question demande des détails spécifiques sur la fiscalité tunisienne (montants exacts, mention exacte dans un document fiscal tel que les notes communes, contenu d'un document, etc.), répondez par 'oui'. \n",
    "    Si la question demande une définition générale ou un concept que vous connaissez, répondez par 'non'. \n",
    "    Répondez SEULEMENT par 'oui' ou 'non'. \n",
    "\n",
    "    Définitions générales :\n",
    "    - **TVA (Taxe sur la Valeur Ajoutée)** : Impôt indirect sur la consommation, appliqué sur la vente de biens et services.\n",
    "    - **Retenue à la source** : Mécanisme fiscal où un tiers prélève un montant sur un revenu et le reverse au fisc.\n",
    "\n",
    "    Exemples :\n",
    "    Question: Qu'est-ce que la TVA ?\n",
    "    Réponse: non\n",
    "\n",
    "    Question: Quel est le taux de TVA applicable aux services de conseil en Tunisie en 2024 ?\n",
    "    Réponse: oui\n",
    "\n",
    "    Question: Comment fonctionne la retenue à la source ?\n",
    "    Réponse: non\n",
    "\n",
    "    Question: Quelle est la référence exacte de la note commune qui traite de la retenue à la source sur les prestations de service ?\n",
    "    Réponse: oui\n",
    "\n",
    "    Question: Quelle est la note commune référence à la TVA ?\n",
    "    Réponse: oui\n",
    "\n",
    "    Question: De quoi parle la note commune numéro 16 ?\n",
    "    Réponse: oui\n",
    "\n",
    "    Question: Quel est le taux de retenue à la source sur les loyers ?\n",
    "    Réponse: oui\n",
    "\n",
    "    Voici la question:\n",
    "    Question: {question}\n",
    "    Réponse:\"\"\"\n",
    "\n",
    "    response = routing_llm.invoke(prompt.format(question=question))\n",
    "    return \"oui\" in response.strip().lower()\n",
    "\n",
    "def load_documents():\n",
    "    conn = sqlite3.connect(\"article52.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT title, content, pub_date, category, applied_year FROM laws\")\n",
    "    documents = []\n",
    "    for row in cursor.fetchall():\n",
    "        metadata = {\"title\": row[0], \"source\": row[2]}\n",
    "        documents.append({\"text\": row[1], \"metadata\": metadata})\n",
    "    conn.close()\n",
    "    return documents\n",
    "\n",
    "def split_documents(documents, chunk_size=2000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        texts = text_splitter.split_text(doc[\"text\"])\n",
    "        for text in texts:\n",
    "            chunks.append({\"text\": text, \"metadata\": doc[\"metadata\"]})\n",
    "    return chunks\n",
    "\n",
    "def init_vector_store(chunks=None):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    persist_dir = \"./chroma_db\"\n",
    "    if os.path.exists(persist_dir) and os.listdir(persist_dir):\n",
    "        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)\n",
    "    if not chunks:\n",
    "        raise ValueError(\"No chunks provided for vector store initialization\")\n",
    "    vector_store = Chroma.from_texts(\n",
    "        texts=[chunk[\"text\"] for chunk in chunks],\n",
    "        embedding=embeddings,\n",
    "        metadatas=[chunk[\"metadata\"] for chunk in chunks],\n",
    "        persist_directory=persist_dir\n",
    "    )\n",
    "    vector_store.persist()\n",
    "    return vector_store\n",
    "\n",
    "# Different prompts for different scenarios\n",
    "retrieval_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|im_start|>system\n",
    "    Vous êtes un expert fiscal tunisien. Répondez en français en vous référant AU CONTEXTE DE LA BASE DES CONNAISSANCES:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|im_start|>system\n",
    "    Vous êtes un assistant tunisien qui répond à des généralités sur la fiscalité tunisienne (définitions TVA, retenue à la source, etc.) sans détails sur les taux en question. \n",
    "    Répondez en français de manière concise et claire, sans inclure de raisonnement ou de formatage supplémentaire et sans mentionner aucun pays sauf si c'est dans la question. \n",
    "    Assurez-vous de raffiner votre réponse à la question posée et veillez à éviter toute confusion entre les impôts directs et indirects.\n",
    "    \n",
    "    Question: {question}\n",
    "    <|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def create_rag_chain(vector_store, llm, routing_llm):\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    def route(inputs):\n",
    "        question = inputs[\"question\"]\n",
    "        # Classify whether retrieval is needed\n",
    "        needs_retrieval = classify_question(routing_llm, question)\n",
    "        print(\"Needs retrieval: \", needs_retrieval)\n",
    "        return {\"question\": question, \"needs_retrieval\": needs_retrieval}\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"context\": lambda x: retriever.invoke(x[\"question\"])\n",
    "        }\n",
    "        | retrieval_prompt\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    general_chain = (\n",
    "        {\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | general_prompt\n",
    "        | routing_llm\n",
    "    )  # Use Phi for general answers\n",
    "\n",
    "    # Use RunnableBranch to conditionally execute the retrieval or general chain\n",
    "    full_chain = RunnableBranch(\n",
    "        (lambda x: x[\"needs_retrieval\"], retrieval_chain),\n",
    "        general_chain\n",
    "    ).with_types(input_type=dict)\n",
    "\n",
    "    return route | full_chain\n",
    "\n",
    "def initialize_system():\n",
    "    persist_dir = \"./chroma_db\"\n",
    "    if not os.path.exists(persist_dir) or not os.listdir(persist_dir):\n",
    "        documents = load_documents()\n",
    "        chunks = split_documents(documents)\n",
    "        vector_store = init_vector_store(chunks)\n",
    "    else:\n",
    "        vector_store = init_vector_store()\n",
    "    \n",
    "    llm = init_llm()\n",
    "    routing_llm = init_routing_llm()\n",
    "    return create_rag_chain(vector_store, llm, routing_llm), vector_store, llm, routing_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    rag_chain, vector_store, llm, routing_llm = initialize_system()\n",
    "    print(\"Système prêt. Posez votre question (tapez 'exit' pour quitter):\")\n",
    "    while True:\n",
    "        question = input(\"\\nQuestion : \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        try:\n",
    "            print(\"Traitement en cours...\")\n",
    "            response = rag_chain.invoke({\"question\": question})\n",
    "            \n",
    "            print(\"\\nRéponse :\")\n",
    "            print(response)\n",
    "            \n",
    "            # Only show the sources if retrieval was done\n",
    "            if \"needs_retrieval\" in response and response[\"needs_retrieval\"]:\n",
    "                print(\"\\nSources utilisées:\")\n",
    "                docs = vector_store.similarity_search(question, k=3)\n",
    "                for i, doc in enumerate(docs, 1):\n",
    "                    print(f\"{i}. {doc.metadata['title']} - {doc.metadata['source']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
