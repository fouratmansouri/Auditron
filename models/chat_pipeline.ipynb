{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfae42b",
   "metadata": {},
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604dba9",
   "metadata": {},
   "source": [
    "This notebook uses the following frameworks\n",
    "* Langchain\n",
    "\n",
    "* Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc67d5",
   "metadata": {},
   "source": [
    "### 1. Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e14b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -qU scikit-learn transformers rank_bm25 langchain_community langchain-qdrant langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f982636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyppeteer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2f29b",
   "metadata": {},
   "source": [
    "### 2. Local models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94575916",
   "metadata": {},
   "source": [
    "Loading the embedding model \"sentence-camembert-large\" from hugging face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482de071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"dangvantuan/sentence-camembert-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dddf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"mistral:7b-instruct\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06eb40",
   "metadata": {},
   "source": [
    "### 3. Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42238797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant = QdrantClient(path=\"../qdrant_data\") #Switched from using qdrant 'production mode' to 'embedded mode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import uuid\n",
    "# import hashlib\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "# # Helper function to convert string IDs to valid UUID-compatible IDs\n",
    "# def string_to_uuid(string_id):\n",
    "#     \"\"\"Convert a string to a deterministic UUID by hashing it\"\"\"\n",
    "#     # Create MD5 hash of the string\n",
    "#     hash_object = hashlib.md5(string_id.encode())\n",
    "#     # Convert to hex\n",
    "#     hex_dig = hash_object.hexdigest()\n",
    "#     # Create a UUID from the hex string\n",
    "#     return uuid.UUID(hex_dig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Load your JSON file\n",
    "# try:\n",
    "#     with open('output_chunks.json', 'r', encoding='utf-8') as f:\n",
    "#         documents = json.load(f)\n",
    "#     print(f\"✓ Loaded {len(documents)} documents from output_chunks.json\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Error: output_chunks.json file not found!\")\n",
    "#     sys.exit(1)\n",
    "# except json.JSONDecodeError:\n",
    "#     print(\"Error: Invalid JSON format in output_chunks.json!\")\n",
    "#     sys.exit(1)\n",
    "\n",
    "# # 5. Create a collection\n",
    "# collection_name = \"Auditron_legal_chunks\"\n",
    "# print(f\"Creating collection '{collection_name}'...\")\n",
    "# qdrant.recreate_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vectors_config=VectorParams(size=model.get_sentence_embedding_dimension(), distance=Distance.COSINE),\n",
    "# )\n",
    "\n",
    "# # 6. Process the documents\n",
    "# print(\"Processing documents...\")\n",
    "# total_chunks = sum(len(doc.get(\"chunks\", [])) for doc in documents)\n",
    "# batch_size = 50\n",
    "# processed_count = 0\n",
    "# error_count = 0\n",
    "# batch_points = []\n",
    "# id_mapping = {}  # To store mapping between original IDs and UUID IDs\n",
    "\n",
    "# for doc_idx, doc in enumerate(documents):\n",
    "#     print(f\"Processing document {doc_idx+1}/{len(documents)}\")\n",
    "#     chunks = doc.get(\"chunks\", [])\n",
    "    \n",
    "#     for chunk_idx, chunk in enumerate(chunks):\n",
    "#         try:\n",
    "#             # Extract text and ID\n",
    "#             text = chunk.get(\"text\", \"\")\n",
    "#             original_id = chunk.get(\"chunk_id\", f\"unknown_{doc_idx}_{chunk_idx}\")\n",
    "            \n",
    "#             if not text.strip():  # Skip empty chunks\n",
    "#                 print(f\"Warning: Empty text in chunk {original_id}\")\n",
    "#                 continue\n",
    "            \n",
    "#             # Clean and truncate text if necessary to avoid index errors\n",
    "#             # Some models have maximum input length limitations\n",
    "#             max_text_length = 512  # Adjust based on your model's limitations\n",
    "#             text = text.strip()[:max_text_length]\n",
    "            \n",
    "#             # Convert string ID to UUID\n",
    "#             point_id = string_to_uuid(original_id)\n",
    "            \n",
    "#             # Save mapping\n",
    "#             id_mapping[str(point_id)] = original_id\n",
    "            \n",
    "#             # Generate embedding with error handling\n",
    "#             try:\n",
    "#                 embedding = model.encode(text, show_progress_bar=False)\n",
    "#             except Exception as embed_error:\n",
    "#                 print(f\"Embedding error for chunk {original_id}: {str(embed_error)}\")\n",
    "#                 # Try with a shorter text if it might be a length issue\n",
    "#                 if len(text) > 200:\n",
    "#                     try:\n",
    "#                         shorter_text = text[:200]\n",
    "#                         print(f\"Retrying with shorter text for {original_id}\")\n",
    "#                         embedding = model.encode(shorter_text, show_progress_bar=False)\n",
    "#                     except Exception as retry_error:\n",
    "#                         print(f\"Still failed with shorter text: {str(retry_error)}\")\n",
    "#                         error_count += 1\n",
    "#                         continue\n",
    "#                 else:\n",
    "#                     error_count += 1\n",
    "#                     continue\n",
    "            \n",
    "#             # Create point with UUID\n",
    "#             point = PointStruct(\n",
    "#                 id=str(point_id),\n",
    "#                 vector=embedding.tolist(),\n",
    "#                 payload={\n",
    "#                     \"text\": text,\n",
    "#                     \"original_id\": original_id,  # Keep original ID in payload\n",
    "#                     \"structures\": chunk.get(\"structures\", []),\n",
    "#                     \"document_path\": chunk.get(\"document_path\", []),\n",
    "#                     \"metadata\": chunk.get(\"metadata\", {})\n",
    "#                 }\n",
    "#             )\n",
    "            \n",
    "#             # Add to batch\n",
    "#             batch_points.append(point)\n",
    "#             processed_count += 1\n",
    "            \n",
    "#             # If batch is full, upload to Qdrant\n",
    "#             if len(batch_points) >= batch_size:\n",
    "#                 qdrant.upsert(\n",
    "#                     collection_name=collection_name,\n",
    "#                     points=batch_points,\n",
    "#                 )\n",
    "#                 print(f\"Uploaded batch: {processed_count}/{total_chunks} chunks ({error_count} errors so far)\")\n",
    "#                 batch_points = []\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing chunk {original_id}: {str(e)}\")\n",
    "#             error_count += 1\n",
    "\n",
    "# # Upload any remaining points\n",
    "# if batch_points:\n",
    "#     qdrant.upsert(\n",
    "#         collection_name=collection_name,\n",
    "#         points=batch_points,\n",
    "#     )\n",
    "#     print(f\"Uploaded final batch: {processed_count}/{total_chunks} chunks\")\n",
    "\n",
    "# # Save ID mapping for reference (optional)\n",
    "# try:\n",
    "#     with open('id_mapping.json', 'w', encoding='utf-8') as f:\n",
    "#         json.dump(id_mapping, f, indent=2)\n",
    "#     print(\"✓ Saved ID mapping to id_mapping.json\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Warning: Could not save ID mapping: {str(e)}\")\n",
    "\n",
    "# print(f\"✅ Successfully processed {processed_count}/{total_chunks} chunks into Qdrant collection '{collection_name}'!\")\n",
    "# print(f\"Total errors encountered: {error_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa01f5b",
   "metadata": {},
   "source": [
    "### 4. Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c868e460",
   "metadata": {},
   "source": [
    "**4.1 Semantic search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2076bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_qdrant import QdrantVectorStore\n",
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.http.models import SearchParams\n",
    "# from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# # 3. Create the vector store with LangChain\n",
    "# vector_store = QdrantVectorStore(\n",
    "#     client=qdrant,\n",
    "#     collection_name=\"Auditron_legal_chunks\",  # Your collection name\n",
    "#     content_payload_key=\"text\", \n",
    "#     embedding=embeddings,\n",
    "# )\n",
    "# #    search_params=SearchParams(hnsw_ef=128)  # Your search params\n",
    "# # 4. Create the retriever from the vector store\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# # 5. Use the retriever\n",
    "# query = \"Quel est le nouveau taux unifié de retenue à la source applicable aux loyers, rémunérations non commerciales, honoraires et commissions en Tunisie depuis l'adoption de la loi N° 2020-46 du 23 décembre 2020?\"\n",
    "# documents = retriever.invoke(query)\n",
    "\n",
    "# # The retrieved documents will include content and metadata\n",
    "# for doc in documents:\n",
    "#     print(doc.page_content)  # Access the content\n",
    "#     print(doc.metadata)      # Access the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7780c0",
   "metadata": {},
   "source": [
    "### Document re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d19070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# ### Retrieval Grader\n",
    "\n",
    "# # Doc grader instructions\n",
    "# doc_grader_instructions = \"\"\"Vous êtes un modèle chargé d’évaluer la pertinence d’un document récupéré par rapport à une question utilisateur.\n",
    "\n",
    "# Si le document contient des mots-clés ou un sens sémantique lié à la question, considérez-le comme pertinent.\"\"\"\n",
    "\n",
    "# # Grader prompt\n",
    "# doc_grader_prompt = \"\"\"Voici le document récupéré : \\n\\n {document} \\n\\n Voici la question de l'utilisateur : \\n\\n {question}.\n",
    "\n",
    "# Évaluez soigneusement et objectivement si le document contient au moins une information pertinente en lien avec la question.\n",
    "\n",
    "# Retournez un JSON avec une clé unique, binary_score, qui sera 'oui' ou 'non' pour indiquer si le document contient au moins une information pertinente pour la question.\"\"\"\n",
    "# # Test\n",
    "# question = \"Quel est le nouveau taux unifié de retenue à la source applicable aux loyers, rémunérations non commerciales, honoraires et commissions en Tunisie depuis l'adoption de la loi N° 2020-46 du 23 décembre 2020?\"\n",
    "\n",
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = docs[0].page_content\n",
    "\n",
    "# doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "#     document=doc_txt, question=question\n",
    "# )\n",
    "# result = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=doc_grader_instructions)]\n",
    "#     + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "# )\n",
    "# json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e749dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = docs[2].page_content\n",
    "\n",
    "# doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "#     document=doc_txt, question=question\n",
    "# )\n",
    "# result = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=doc_grader_instructions)]\n",
    "#     + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "# )\n",
    "# json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f6ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbde3f0",
   "metadata": {},
   "source": [
    "**4.2 Implementig Hybrid Search threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841cf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\foura\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name dangvantuan/sentence-camembert-large. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 [Score: 0.630]\n",
      "Pour la retenue à la source Suite à l'augmentation du taux de l'impôt sur les sociétés de 15% à 20%, les taux de retenue à la source dus au titre des opérations de cession par les personnes morales non résidentes non établies en Tunisie de biens immobiliers situés en Tunisie ou de droits y afférents, ou de droits sociaux dans les sociétés civiles immobilières, ainsi que de titres ou droits y afférents ou de leur rétrocession, réalisés en Tunisie, ont été adaptés par leur augmentation de: 10% à 15% pour le p\n",
      "\n",
      "Document 2 [Score: 0.622]\n",
      "10%(1) au titre des honoraires, commissions, courtages, loyers et rémunérations des activités non commerciales qu’elle qu’en soit l’appellation payés par l’Etat, les collectivités locales, les personnes morales ainsi que les personnes physiques soumises à l’impôt sur le revenu selon le régime réel et les personnes visées au paragraphe II de l’article 22 du présent code. (Modifié Art 69-1 LF 2004-90 du 31/12/2004, Art.45-1 LF 2012-27 du 29/12/2012 et Art 14-5 LF 2020-46 du 23/12/2020). (1) Ce taux s’applique\n",
      "\n",
      "Document 3 [Score: 0.616]\n",
      "10%(1) au titre des honoraires, commissions, courtages, loyers et rémunérations des activités non commerciales qu’elle qu’en soit l’appellation payés par l’Etat, les collectivités locales, les personnes morales ainsi que les personnes physiques soumises à l’impôt sur le revenu selon le régime réel et les personnes visées au paragraphe II de l’article 22 du présent code. (Modifié Art 69-1 LF 2004-90 du 31/12/2004, Art.45-1 LF 2012-27 du 29/12/2012 et Art 14-5 LF 2020-46 du 23/12/2020). Le taux de 10%(1) s’ap\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foura\\AppData\\Local\\Temp\\ipykernel_11052\\1021307685.py:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.vectorstore.client.search(\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List, Any\n",
    "from pydantic import Field\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. Updated Qdrant retriever with explicit embeddings\n",
    "class QdrantScoreRetriever(BaseRetriever):\n",
    "    vectorstore: QdrantVectorStore = Field(...)\n",
    "    embeddings: Embeddings = Field(...)  # Explicit embeddings field\n",
    "    k: int = Field(default=5)\n",
    "    score_threshold: float = Field(default=0.4)\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Get embedding using the explicit embeddings model\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Search Qdrant with score threshold\n",
    "        results = self.vectorstore.client.search(\n",
    "            collection_name=self.vectorstore.collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=self.k,\n",
    "            score_threshold=self.score_threshold,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for result in results:\n",
    "            content = result.payload.get(\"text\", \"\")\n",
    "            metadata = {k: v for k, v in result.payload.items() if k != \"text\"}\n",
    "            metadata[\"score\"] = result.score  # Store similarity score\n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "        \n",
    "        return documents\n",
    "\n",
    "# 2. Initialize components\n",
    "qdrant = QdrantClient(path=\"../qdrant_data\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"dangvantuan/sentence-camembert-large\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant,\n",
    "    collection_name=\"Auditron_legal_chunks\",\n",
    "    content_payload_key=\"text\",\n",
    "    embedding=embeddings,  # This might not be needed depending on your Qdrant setup\n",
    ")\n",
    "\n",
    "# 3. Create Qdrant retriever with explicit embeddings\n",
    "qdrant_retriever = QdrantScoreRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    embeddings=embeddings,  # Pass embeddings explicitly\n",
    "    k=5,\n",
    "    score_threshold=0.4\n",
    ")\n",
    "\n",
    "# Load BM25 documents\n",
    "response = qdrant.scroll(\n",
    "    collection_name=\"Auditron_legal_chunks\",\n",
    "    limit=10_000,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "all_documents = [\n",
    "    Document(\n",
    "        page_content=point.payload.get(\"text\", \"\"),\n",
    "        metadata={k: v for k, v in point.payload.items() if k != \"text\"}\n",
    "    )\n",
    "    for point in response[0]\n",
    "]\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    all_documents,\n",
    "    k=5,\n",
    "    with_score=True\n",
    ")\n",
    "\n",
    "# 4. Create ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, qdrant_retriever],\n",
    "    weights=[0.4, 0.6]\n",
    ")\n",
    "\n",
    "# 5. Search function with threshold\n",
    "def retrieve_with_threshold(query: str, threshold: float = 0.4) -> List[Document]:\n",
    "    \"\"\"Retrieve documents with combined score above threshold\"\"\"\n",
    "    docs = ensemble_retriever.invoke(query)\n",
    "    \n",
    "    # Filter documents by combined score\n",
    "    filtered_docs = [\n",
    "        doc for doc in docs\n",
    "        if doc.metadata.get(\"score\", 0) >= threshold\n",
    "    ]\n",
    "    \n",
    "    # Print results\n",
    "    for i, doc in enumerate(filtered_docs, 1):\n",
    "        print(f\"Document {i} [Score: {doc.metadata['score']:.3f}]\")\n",
    "        print(doc.page_content + \"\\n\")\n",
    "    \n",
    "    return filtered_docs\n",
    "\n",
    "# 6. Execute the search\n",
    "results = retrieve_with_threshold(\n",
    "    \"Quel est le nouveau taux unifié de retenue à la source applicable aux loyers, rémunérations non commerciales, honoraires et commissions en Tunisie depuis l'adoption de la loi N° 2020-46 du 23 décembre 2020?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1200abc",
   "metadata": {},
   "source": [
    "### 5. Web search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tavily web-search\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Define allowed domains for Tunisian governmental financial institutions\n",
    "tunisian_gov_domains = [\n",
    "    \"finances.gov.tn\",        # Ministry of Finance\n",
    "    \"douane.gov.tn\",          # Tunisian Customs\n",
    "    \"impots.finances.gov.tn\", # Tax authority\n",
    "    \"cga.gov.tn\",             # General Committee of Insurance\n",
    "    \"cmf.tn\",                 # Financial Market Council\n",
    "    \"portail.finances.gov.tn\" # Finance Ministry Portal\n",
    "    \"swiver.io\"\n",
    "]\n",
    "\n",
    "# Create the search tool with domain filtering\n",
    "web_search_tool = TavilySearchResults(\n",
    "    k=3,\n",
    "    include_domains=tunisian_gov_domains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13e903",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[0;32m      3\u001b[0m docs \u001b[38;5;241m=\u001b[39m web_search_tool\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[1;32m----> 4\u001b[0m docs_txt \u001b[38;5;241m=\u001b[39m format_docs(docs)\n\u001b[0;32m      5\u001b[0m rag_prompt_formatted \u001b[38;5;241m=\u001b[39m rag_prompt\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mdocs_txt, question\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m      6\u001b[0m generation \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke([HumanMessage(content\u001b[38;5;241m=\u001b[39mrag_prompt_formatted)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'format_docs' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"retenue à la source appliqué sur les honoraires en 2021?\"\n",
    "# Test\n",
    "docs = web_search_tool.invoke(query)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=query)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e18a2",
   "metadata": {},
   "source": [
    "### 6. Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc2362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"Vous êtes un assistant pour des tâches de question-réponse.\n",
    "\n",
    "Voici le contexte à utiliser pour répondre à la question :\n",
    "\n",
    "{context}\n",
    "\n",
    "Réfléchissez soigneusement au contexte ci-dessus.\n",
    "\n",
    "Maintenant, examinez la question de l'utilisateur :\n",
    "\n",
    "{question}\n",
    "\n",
    "Fournissez une réponse à cette question en utilisant uniquement le contexte ci-dessus.\n",
    "\n",
    "Utilisez un maximum de trois phrases et gardez la réponse concise.\n",
    "\n",
    "Réponse :\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Test\n",
    "query = '''TEJ'''\n",
    "docs = ensemble_retriever.invoke(query)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=query)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d682cc",
   "metadata": {},
   "source": [
    "Hallucination grader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8372b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Hallucination grader instructions\n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "Vous êtes un enseignant en train de corriger un quiz.\n",
    "\n",
    "Vous recevrez des FAITS et une RÉPONSE D'ÉLÈVE.\n",
    "\n",
    "Voici les critères de notation à suivre :\n",
    "\n",
    "(1) Assurez-vous que la RÉPONSE DE L'ÉLÈVE est bien fondée sur les FAITS.\n",
    "\n",
    "(2) Assurez-vous que la RÉPONSE DE L'ÉLÈVE ne contient pas d'informations « hallucinées » qui sortent du cadre des FAITS.\n",
    "\n",
    "Note :\n",
    "\n",
    "Une note de oui signifie que la réponse de l'élève respecte tous les critères. C’est la note la plus élevée (meilleure).\n",
    "\n",
    "Une note de non signifie que la réponse de l'élève ne respecte pas tous les critères. C’est la note la plus basse que vous pouvez attribuer.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape afin de garantir la justesse de votre raisonnement et de votre conclusion.\n",
    "\n",
    "Évitez d’énoncer directement la bonne réponse dès le départ.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FAITS : \\n\\n {documents} \\n\\n RÉPONSE DE L'ÉLÈVE : {generation}.\n",
    "\n",
    "Retournez un JSON avec deux clés :  \n",
    "- binary_score : une valeur 'oui' ou 'non' indiquant si la RÉPONSE DE L'ÉLÈVE est bien fondée sur les FAITS.  \n",
    "- explanation : une explication justifiant la note attribuée.\n",
    "\"\"\"\n",
    "\n",
    "# Test using documents and generation from above\n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "    documents=docs_txt, generation=generation.content\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=hallucination_grader_instructions)]\n",
    "    + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46427e26",
   "metadata": {},
   "source": [
    "Answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Answer grader instructions\n",
    "answer_grader_instructions = \"\"\"Vous êtes un enseignant en train de corriger un quiz.\n",
    "\n",
    "Vous recevrez une QUESTION et une RÉPONSE D'ÉLÈVE.\n",
    "\n",
    "Voici les critères de notation à suivre :\n",
    "\n",
    "(1) La RÉPONSE DE L'ÉLÈVE aide à répondre à la QUESTION.\n",
    "\n",
    "Note :\n",
    "\n",
    "Une note de oui signifie que la réponse de l'élève respecte tous les critères. C’est la note la plus élevée (meilleure).\n",
    "\n",
    "L’élève peut recevoir une note de oui même si la réponse contient des informations supplémentaires qui ne sont pas explicitement demandées dans la question.\n",
    "\n",
    "Une note de non signifie que la réponse de l'élève ne respecte pas tous les critères. C’est la note la plus basse que vous pouvez attribuer.\n",
    "\n",
    "Expliquez votre raisonnement étape par étape afin de garantir la justesse de votre raisonnement et de votre conclusion.\n",
    "\n",
    "Évitez d’énoncer directement la bonne réponse dès le départ.\n",
    "\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "answer_grader_prompt = \"\"\"QUESTION : \\n\\n {question} \\n\\n RÉPONSE DE L'ÉLÈVE : {generation}.\n",
    "\n",
    "Retournez un JSON avec deux clés :  \n",
    "- binary_score : une valeur 'oui' ou 'non' indiquant si la RÉPONSE DE L'ÉLÈVE respecte les critères.  \n",
    "- explanation : une explication justifiant la note attribuée.\n",
    "\"\"\"\n",
    "\n",
    "# Test\n",
    "answer = generation.content\n",
    "\n",
    "# Test using question and generation from above\n",
    "answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "    question=question, generation=answer\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=answer_grader_instructions)]\n",
    "    + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d410f",
   "metadata": {},
   "source": [
    "### 7. ChatAgent with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d44a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated, Optional, Dict\n",
    "\n",
    "class ConversationState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state for the Withholding Tax RAG Pipeline containing information\n",
    "    propagated through each node in the workflow.\n",
    "    \"\"\"\n",
    "    # User input\n",
    "    original_query: str  # Original user question about withholding taxes\n",
    "    \n",
    "    # Query re-writing step\n",
    "    rewritten_query: str  # Processed query optimized for retrieval\n",
    "    \n",
    "    # Retrieval step\n",
    "    retrieved_documents: List[str]  # Documents from hybrid search (BM25 + semantic)\n",
    "    relevancy_scores: Optional[List[float]]  # Relevancy scores for retrieved documents\n",
    "    \n",
    "    # LLM Document Filtering step - Binary relevance (relevant/irrelevant)\n",
    "    filtered_documents: List[str]  # Only documents classified as relevant by LLM\n",
    "    document_relevance: Dict[str, bool]  # Mapping document to relevance (True=relevant, False=irrelevant)\n",
    "    filter_reasoning: Optional[Dict[str, str]]  # LLM reasoning for each document's relevance decision\n",
    "    \n",
    "    # Response generation step\n",
    "    generated_response: str  # LLM generated response from filtered documents\n",
    "    \n",
    "    # Response validation step\n",
    "    validation_result: str  # \"Yes\" or \"No\" - whether response answers the query\n",
    "    validation_reason: Optional[str]  # Explanation for the validation decision\n",
    "    \n",
    "    # Fallback & retry mechanism\n",
    "    retry_count: Annotated[int, operator.add]  # Track number of retry attempts\n",
    "    max_retries: int  # Maximum number of retry attempts (default: 3)\n",
    "    web_search_results: Optional[List[str]]  # Results from web search fallback\n",
    "    \n",
    "    # Final response\n",
    "    final_response: str  # Final validated response to be returned to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93316737",
   "metadata": {},
   "source": [
    "**Defining the nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec12372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langgraph.graph import END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "def query_rewriting(state):\n",
    "    \"\"\"\n",
    "    Process and rewrite the initial user query for optimal retrieval.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state with original_query\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with rewritten_query\n",
    "    \"\"\"\n",
    "    print(\"---QUERY REWRITING---\")\n",
    "    original_query = state[\"original_query\"]\n",
    "    \n",
    "    # Rewrite query for optimal retrieval\n",
    "    rewritten_query = llm.invoke([\n",
    "        SystemMessage(content=\"Tu es un expert en reformulation de prompts. \"\n",
    "        \"Ton travail est de prendre un prompt complexe, de le simplifier, de le rendre clair, direct et facile à comprendre. \"\n",
    "        \"Le résultat doit être en français parfait. Ne change pas le sens du prompt, seulement rends-le plus simple et clair.\"\n",
    "        \"répond juste par la question.\"),\n",
    "        HumanMessage(content=original_query)\n",
    "    ])\n",
    "    \n",
    "    return {\"rewritten_query\": rewritten_query.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c579b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foura\\AppData\\Local\\Temp\\ipykernel_11052\\155685459.py:6: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_llm, temperature=0)\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize Bloom 500 model\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "def filter_documents_by_relevance(query: str, documents: List[Any]) -> Tuple[List[Any], Dict[str, bool], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Filter documents based on their relevance to the query using a LLM in JSON mode.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to evaluate document relevance against\n",
    "        documents (List[Any]): List of document objects to evaluate\n",
    "        llm_json_mode: The language model that supports JSON output mode\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - filtered_documents (List[Any]): Only the relevant documents\n",
    "        - document_relevance (Dict[str, bool]): Mapping of document IDs to relevance decision\n",
    "        - filter_reasoning (Dict[str, str]): Reasoning for each relevance decision\n",
    "    \"\"\"\n",
    "    filtered_documents = []\n",
    "    document_relevance = {}\n",
    "    filter_reasoning = {}\n",
    "    \n",
    "    # Doc grader instructions\n",
    "    doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "    # Grader prompt template\n",
    "    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Get document content and ID\n",
    "        doc_content = doc.page_content\n",
    "        doc_id = doc.metadata.get(\"id\", str(hash(doc_content)))\n",
    "        \n",
    "        # Format the grader prompt with the document content and query\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "            document=doc_content, \n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Get LLM response using the system and human message approach\n",
    "        response = llm_json_mode.invoke(\n",
    "            [SystemMessage(content=doc_grader_instructions)] + \n",
    "            [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "        )\n",
    "        \n",
    "        # Extract the text content and store the raw response\n",
    "        response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "        filter_reasoning[doc_id] = response_text\n",
    "        \n",
    "        try:\n",
    "            # Parse the JSON response to determine relevance\n",
    "            parsed_response = json.loads(response_text)\n",
    "            is_relevant = parsed_response.get(\"binary_score\", \"\").lower() == \"yes\"\n",
    "            \n",
    "            # Store relevance result\n",
    "            document_relevance[doc_id] = is_relevant\n",
    "            \n",
    "            # Add to filtered documents if relevant\n",
    "            if is_relevant:\n",
    "                filtered_documents.append(doc)\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            # Handle case where response is not valid JSON\n",
    "            document_relevance[doc_id] = False\n",
    "            filter_reasoning[doc_id] += \" (Error: Response not in valid JSON format)\"\n",
    "    \n",
    "    return filtered_documents, document_relevance, filter_reasoning\n",
    "\n",
    "def retrieval_with_filtering(state):\n",
    "    \"\"\"\n",
    "    Perform hybrid search (BM25 + semantic) using vector DB with relevancy threshold,\n",
    "    then filter documents using Bloom LLM for binary relevance assessment.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state with rewritten_query\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with retrieved documents, relevancy scores, and filtering results\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVAL WITH LLM FILTERING---\")\n",
    "    print(f\"Processing query: {state['rewritten_query']}\")\n",
    "    query = state[\"rewritten_query\"]\n",
    "    \n",
    "    # Hybrid search using BM25 + semantic\n",
    "    documents = retrieve_with_threshold(query)\n",
    "    print(f\"Retrieved {len(documents)} documents\")\n",
    "    \n",
    "    # Calculate relevancy scores\n",
    "    relevancy_scores = [doc.metadata.get(\"score\", 0.0) for doc in documents]\n",
    "    \n",
    "    # Filter documents by relevance using Bloom LLM\n",
    "    filtered_docs, doc_relevance, reasoning = filter_documents_by_relevance(query, documents)\n",
    "    print(f\"Filtered to {len(filtered_docs)} relevant documents\")\n",
    "    \n",
    "    # Create doc_id to document mapping for accessing original documents\n",
    "    doc_id_mapping = {doc.metadata.get(\"id\", str(hash(doc.page_content))): doc for doc in documents}\n",
    "    \n",
    "    # Format document relevance map to use actual documents as keys if needed\n",
    "    document_relevance = {str(doc_id): relevance for doc_id, relevance in doc_relevance.items()}\n",
    "    \n",
    "    # Add metadata about filtering\n",
    "    processing_metadata = {\n",
    "        \"processed_by\": \"fouratmansouri\",\n",
    "        \"processing_timestamp\": \"2025-05-09 01:05:46\",\n",
    "        \"total_documents\": len(documents),\n",
    "        \"relevant_documents\": len(filtered_docs)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"retrieved_documents\": documents,\n",
    "        \"relevancy_scores\": relevancy_scores,\n",
    "        \"filtered_documents\": filtered_docs,\n",
    "        \"document_relevance\": document_relevance,\n",
    "        \"filter_reasoning\": reasoning,\n",
    "        \"processing_metadata\": processing_metadata\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5278d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c44a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_generation(state):\n",
    "    \"\"\"\n",
    "    LLM processes filtered documents to generate a response.\n",
    "    No longer uses all retrieved documents as fallback.\n",
    "    Uses web search as fallback when high-quality documents aren't available.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state with rewritten_query, retrieved_documents,\n",
    "                     and filtered_documents from LLM relevance filtering\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with generated_response\n",
    "    \"\"\"\n",
    "    print(\"---RESPONSE GENERATION---\")\n",
    "    current_time = \"2025-05-09 12:30:02\"\n",
    "    user_login = \"fouratmansouri\"\n",
    "    print(f\"Process initiated by {user_login} at {current_time}\")\n",
    "    \n",
    "    query = state[\"rewritten_query\"]\n",
    "    \n",
    "    # First check for filtered documents\n",
    "    if \"filtered_documents\" in state and state[\"filtered_documents\"]:\n",
    "        documents = state[\"filtered_documents\"]\n",
    "        print(f\"Using {len(documents)} filtered documents for response generation\")\n",
    "    \n",
    "    # Instead of using all retrieved documents, get only those above threshold\n",
    "    else:\n",
    "        print(\"No filtered documents available, extracting high-quality documents\")\n",
    "        # Define threshold for document quality\n",
    "        threshold_value = 0.7  # Adjust based on your scoring system\n",
    "        \n",
    "        # Get retrieved documents\n",
    "        retrieved_docs = state.get(\"retrieved_documents\", [])\n",
    "        \n",
    "        # Filter only high-quality documents\n",
    "        high_quality_docs = []\n",
    "        for doc in retrieved_docs:\n",
    "            # Check if document meets quality threshold\n",
    "            if doc.metadata.get('relevance_score', 0) >= threshold_value:\n",
    "                high_quality_docs.append(doc)\n",
    "        \n",
    "        documents = high_quality_docs\n",
    "        print(f\"Using {len(documents)} high-quality documents for response generation\")\n",
    "        \n",
    "        # If no high-quality documents found or documents are insufficient, try web search\n",
    "        if not documents or len(documents) < 2:  # Adjust minimum document threshold as needed\n",
    "            print(\"Insufficient high-quality documents, attempting web search\")\n",
    "            \n",
    "            # Perform web search to get additional information\n",
    "            try:\n",
    "                web_docs = web_search_tool.invoke({\"query\": query})\n",
    "                web_results = [Document(page_content=d[\"content\"]) for d in web_docs]\n",
    "                print(f\"Retrieved {len(web_results)} documents from web search\")\n",
    "                \n",
    "                # Combine existing high-quality documents with web results\n",
    "                documents = high_quality_docs + web_results\n",
    "                print(f\"Using combined {len(documents)} documents for response generation\")\n",
    "                \n",
    "                # Still check if we have enough data\n",
    "                if not documents:\n",
    "                    raise ValueError(\"No documents found from web search either\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Web search failed: {str(e)}\")\n",
    "                # If web search fails or returns no results, return informative response\n",
    "                insufficient_data_response = f\"Je n'ai pas trouvé d'informations suffisamment pertinentes concernant '{query}', même après recherche sur le web. Pourriez-vous reformuler votre question ou fournir plus de détails?\"\n",
    "                \n",
    "                response_metadata = {\n",
    "                    \"total_documents\": len(retrieved_docs),\n",
    "                    \"relevant_documents\": 0,\n",
    "                    \"generated_by\": user_login,\n",
    "                    \"generation_timestamp\": current_time,\n",
    "                    \"status\": \"insufficient_data\",\n",
    "                    \"web_search_attempted\": True,\n",
    "                    \"web_search_successful\": False\n",
    "                }\n",
    "                \n",
    "                return {\n",
    "                    \"generated_response\": insufficient_data_response,\n",
    "                    \"response_metadata\": response_metadata,\n",
    "                    \"insufficient_data\": True\n",
    "                }\n",
    "    \n",
    "    # Format documents for context\n",
    "    docs_txt = format_docs(documents)\n",
    "    \n",
    "    rag_prompt = \"\"\"Vous êtes un assistant pour des tâches de question-réponse.\n",
    "\n",
    "Voici le contexte à utiliser pour répondre à la question :\n",
    "\n",
    "{context}\n",
    "\n",
    "Réfléchissez soigneusement au contexte ci-dessus.\n",
    "\n",
    "Maintenant, examinez la question de l'utilisateur :\n",
    "\n",
    "{question}\n",
    "\n",
    "Fournissez une réponse à cette question en utilisant uniquement le contexte ci-dessus.\n",
    "\n",
    "Utilisez un maximum de trois phrases et gardez la réponse concise.\n",
    "\n",
    "Réponse :\"\"\"\n",
    "    # Generate response using RAG\n",
    "    rag_prompt_formatted = rag_prompt.format(\n",
    "        context=docs_txt, \n",
    "        question=query\n",
    "    )\n",
    "    \n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    print(generation.content)\n",
    "    \n",
    "    # Determine source of documents used\n",
    "    web_search_used = \"web_search_results\" in state or any(getattr(doc, 'source', '') == 'web_search' for doc in documents)\n",
    "    \n",
    "    # Add metadata about document filtering to the response\n",
    "    response_metadata = {\n",
    "        \"total_documents\": len(state.get(\"retrieved_documents\", [])),\n",
    "        \"relevant_documents\": len(documents),\n",
    "        \"generated_by\": user_login,\n",
    "        \"generation_timestamp\": current_time,\n",
    "        \"status\": \"success\",\n",
    "        \"web_search_used\": web_search_used\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"generated_response\": generation.content,\n",
    "        \"response_metadata\": response_metadata,\n",
    "        \"documents_used\": documents,  # Track which documents were actually used\n",
    "        \"web_search_used\": web_search_used  # Flag if web search was used\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_validation(state):\n",
    "    \"\"\"\n",
    "    LLM judge evaluates if response answers the re-written query.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Updated state with validation result and reason\n",
    "    \"\"\"\n",
    "    print(\"---RESPONSE VALIDATION---\")\n",
    "    rewritten_query = state[\"rewritten_query\"]\n",
    "    generated_response = state[\"generated_response\"]\n",
    "    \n",
    "    # Answer grader instructions\n",
    "    answer_grader_instructions = \"\"\"Vous êtes un enseignant en train de corriger un quiz.\n",
    "Vous recevrez une QUESTION et une RÉPONSE D'ÉLÈVE.\n",
    "Voici les critères de notation à suivre :\n",
    "(1) La RÉPONSE DE L'ÉLÈVE aide à répondre à la QUESTION.\n",
    "Note :\n",
    "Une note de oui signifie que la réponse de l'élève respecte tous les critères. C'est la note la plus élevée (meilleure).\n",
    "L'élève peut recevoir une note de oui même si la réponse contient des informations supplémentaires qui ne sont pas explicitement demandées dans la question.\n",
    "Une note de non signifie que la réponse de l'élève ne respecte pas tous les critères. C'est la note la plus basse que vous pouvez attribuer.\n",
    "Expliquez votre raisonnement étape par étape afin de garantir la justesse de votre raisonnement et de votre conclusion.\n",
    "Évitez d'énoncer directement la bonne réponse dès le départ.\n",
    "\"\"\"\n",
    "    # Grader prompt\n",
    "    answer_grader_prompt = \"\"\"QUESTION : \\n\\n {rewritten_query} \\n\\n RÉPONSE DE L'ÉLÈVE : {generated_response}.\n",
    "Retournez un JSON avec deux clés :  \n",
    "- binary_score : une valeur 'oui' ou 'non' indiquant si la RÉPONSE DE L'ÉLÈVE respecte les critères.  \n",
    "- explanation : une explication justifiant la note attribuée.\n",
    "\"\"\"\n",
    "    # Format the prompt correctly using the state variables\n",
    "    answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "        rewritten_query=rewritten_query, \n",
    "        generated_response=generated_response\n",
    "    )\n",
    "    \n",
    "    # Call the LLM\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=answer_grader_instructions)]\n",
    "        + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    validation_result = json.loads(result.content)\n",
    "    \n",
    "    return {\n",
    "        \"validation_result\": validation_result[\"binary_score\"],  # \"oui\" or \"non\"\n",
    "        \"validation_reason\": validation_result[\"explanation\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback_retry(state):\n",
    "    \"\"\"\n",
    "    Fall back to web search for additional content when validation fails.\n",
    "    Implements retry loop with maximum 3 attempts.\n",
    "    Combines web search results with only the documents that passed the threshold.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with web_search_results and incremented retry_count\n",
    "    \"\"\"\n",
    "    print(\"---FALLBACK & RETRY---\")\n",
    "    rewritten_query = state[\"rewritten_query\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # Check if we've reached max retries\n",
    "    if retry_count >= state[\"max_retries\"]:\n",
    "        # If max retries reached, prepare final response acknowledging limitations\n",
    "        final_response = f\"Après plusieurs tentatives, je n'arrive pas à trouver une réponse complète concernant '{rewritten_query}'. Veuillez envisager de reformuler votre question ou de consulter un professionnel de la fiscalité pour des conseils spécifiques concernant la retenue à la source.\"\n",
    "        return {\n",
    "            \"retry_count\": retry_count + 1,\n",
    "            \"final_response\": final_response\n",
    "        }\n",
    "    \n",
    "    # Perform web search to get additional information\n",
    "    web_docs = web_search_tool.invoke({\"query\": rewritten_query})\n",
    "    web_results = [Document(page_content=d[\"content\"]) for d in web_docs]\n",
    "    \n",
    "    # Get current documents and filter only those above threshold\n",
    "    current_docs = state.get(\"retrieved_documents\", [])\n",
    "    above_threshold_docs = state.get(\"above_threshold_docs\", [])\n",
    "    \n",
    "    # If above_threshold_docs is not available in state, we need to compute it\n",
    "    if not above_threshold_docs and current_docs:\n",
    "        # This assumes you have a way to determine which docs are above threshold\n",
    "        # If you don't have this in state, you'll need to add logic to filter them here\n",
    "        # For example, if you have relevance scores:\n",
    "        # above_threshold_docs = [doc for doc in current_docs if doc.metadata.get('relevance_score', 0) >= threshold_value]\n",
    "        pass\n",
    "    \n",
    "    # Combine web results with documents above threshold\n",
    "    combined_docs = above_threshold_docs + web_results\n",
    "    \n",
    "    return {\n",
    "        \"retrieved_documents\": combined_docs,\n",
    "        \"web_search_results\": web_results,\n",
    "        \"above_threshold_docs\": above_threshold_docs,  # Keep track of above-threshold docs\n",
    "        \"retry_count\": retry_count + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a246978",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edge Functions\n",
    "def does_response_answer_query(state):\n",
    "    \"\"\"\n",
    "    Determines whether the response answers the rewritten query based on validation.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: \"oui\" if response answers query, \"non\" if not\n",
    "    \"\"\"\n",
    "    print(\"---DECISION POINT: DOES RESPONSE ANSWER QUERY?---\")\n",
    "    \n",
    "    validation_result = state[\"validation_result\"]\n",
    "    \n",
    "    if validation_result == \"oui\":\n",
    "        print(\"---DECISION: RESPONSE VALIDATED SUCCESSFULLY---\")\n",
    "        # Set final response to return to user\n",
    "        state[\"final_response\"] = state[\"generated_response\"]\n",
    "        return \"oui\"\n",
    "    else:\n",
    "        print(f\"---DECISION: VALIDATION FAILED - {state['validation_reason']}---\")\n",
    "        return \"non\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c60976",
   "metadata": {},
   "source": [
    "**Full graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this once before rendering\n",
    "# import pyppeteer.chromium_downloader\n",
    "# pyppeteer.chromium_downloader.download_chromium()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "import logging\n",
    "\n",
    "def configure_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Add file handler to save logs to file\n",
    "    file_handler = logging.FileHandler(\"conversation_system.log\")\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def build_graph():\n",
    "    logger = configure_logging()\n",
    "    logger.info(\"Building Withholding Tax RAG Pipeline\")\n",
    "    \n",
    "    workflow = StateGraph(ConversationState)\n",
    "    \n",
    "    # Define the nodes according to the schema\n",
    "    workflow.add_node(\"query_rewriting\", query_rewriting)                    # 2. Query Re-writing\n",
    "    workflow.add_node(\"retrieval_with_filtering\", retrieval_with_filtering)  # 3. Combined Retrieval & Filtering \n",
    "    workflow.add_node(\"response_generation\", response_generation)            # 4. Response Generation\n",
    "    workflow.add_node(\"response_validation\", response_validation)            # 5. Response Validation\n",
    "    workflow.add_node(\"fallback_retry\", fallback_retry)                      # 6. Fallback & Retry\n",
    "    \n",
    "    # Build graph\n",
    "    # Start with user query (1) then goes to query rewriting (2)\n",
    "    workflow.set_entry_point(\"query_rewriting\")\n",
    "    \n",
    "    # Linear flow from query rewriting to retrieval with filtering\n",
    "    workflow.add_edge(\"query_rewriting\", \"retrieval_with_filtering\")\n",
    "    \n",
    "    # Retrieval with filtering to response generation\n",
    "    workflow.add_edge(\"retrieval_with_filtering\", \"response_generation\")\n",
    "    \n",
    "    # Response generation to validation\n",
    "    workflow.add_edge(\"response_generation\", \"response_validation\")\n",
    "    \n",
    "    # Conditional edge from validation\n",
    "    workflow.add_conditional_edges(\n",
    "        \"response_validation\",\n",
    "        does_response_answer_query,\n",
    "        {\n",
    "            \"oui\": END,  # Return validated response to user\n",
    "            \"non\": \"fallback_retry\"  # Go to fallback mechanism\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Fallback can retry the retrieval+filtering step\n",
    "    workflow.add_edge(\"fallback_retry\", \"retrieval_with_filtering\")\n",
    "    \n",
    "    # Compile\n",
    "    logger.info(\"Compiling Withholding Tax RAG Pipeline with LLM Document Filtering\")\n",
    "    logger.info(f\"Pipeline built by {{'user': 'fouratmansouri', 'timestamp': '2025-05-09 01:02:53'}}\")\n",
    "    graph = workflow.compile()\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394fb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:36:52,965 - __main__ - INFO - Building Withholding Tax RAG Pipeline\n",
      "2025-05-09 13:36:52,967 - __main__ - INFO - Compiling Withholding Tax RAG Pipeline with LLM Document Filtering\n",
      "2025-05-09 13:36:52,968 - __main__ - INFO - Pipeline built by {'user': 'fouratmansouri', 'timestamp': '2025-05-09 01:02:53'}\n",
      "2025-05-09 13:36:52,979 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---QUERY REWRITING---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:37:00,174 - urllib3.connectionpool - DEBUG - http://localhost:11434 \"POST /api/chat HTTP/1.1\" 200 None\n",
      "C:\\Users\\foura\\AppData\\Local\\Temp\\ipykernel_11052\\1021307685.py:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = self.vectorstore.client.search(\n",
      "2025-05-09 13:37:02,745 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.tavily.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVAL WITH LLM FILTERING---\n",
      "Processing query:  Qu'est-ce que la plateforme TEJ ?\n",
      "Retrieved 0 documents\n",
      "Filtered to 0 relevant documents\n",
      "---RESPONSE GENERATION---\n",
      "Process initiated by fouratmansouri at 2025-05-09 12:30:02\n",
      "No filtered documents available, extracting high-quality documents\n",
      "Using 0 high-quality documents for response generation\n",
      "Insufficient high-quality documents, attempting web search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:37:04,840 - urllib3.connectionpool - DEBUG - https://api.tavily.com:443 \"POST /search HTTP/1.1\" 200 3093\n",
      "2025-05-09 13:37:04,847 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents from web search\n",
      "Using combined 5 documents for response generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:37:21,764 - urllib3.connectionpool - DEBUG - http://localhost:11434 \"POST /api/chat HTTP/1.1\" 200 None\n",
      "2025-05-09 13:37:31,843 - httpcore.connection - DEBUG - close.started\n",
      "2025-05-09 13:37:31,845 - httpcore.connection - DEBUG - close.complete\n",
      "2025-05-09 13:37:31,845 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None\n",
      "2025-05-09 13:37:31,847 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D458DD8290>\n",
      "2025-05-09 13:37:31,847 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-05-09 13:37:31,848 - httpcore.http11 - DEBUG - send_request_headers.complete\n",
      "2025-05-09 13:37:31,849 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-05-09 13:37:31,849 - httpcore.http11 - DEBUG - send_request_body.complete\n",
      "2025-05-09 13:37:31,850 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La plateforme TEJ est une plateforme utilisée pour consulter et effectuer des services depuis votre ordinateur, notamment en matière de télé-liquidation et télé-amende.\n",
      "---RESPONSE VALIDATION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:37:38,375 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Fri, 09 May 2025 12:37:38 GMT'), (b'Transfer-Encoding', b'chunked')])\n",
      "2025-05-09 13:37:38,375 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-05-09 13:37:38,376 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-05-09 13:38:01,981 - httpcore.http11 - DEBUG - receive_response_body.complete\n",
      "2025-05-09 13:38:01,981 - httpcore.http11 - DEBUG - response_closed.started\n",
      "2025-05-09 13:38:01,981 - httpcore.http11 - DEBUG - response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION POINT: DOES RESPONSE ANSWER QUERY?---\n",
      "---DECISION: RESPONSE VALIDATED SUCCESSFULLY---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    graph = build_graph()\n",
    "    # Initialize the state with a user query\n",
    "    initial_state = {\"original_query\": \"c'est quoi la plateforme TEJ ?\"}\n",
    "    \n",
    "    # Execute the graph\n",
    "    for state in graph.stream(initial_state):\n",
    "        step = state.get(\"__step__\", \"\")\n",
    "        if step:\n",
    "            print(f\"Step: {step}\")\n",
    "            if step == \"response_generation\":\n",
    "                print(f\"Response: {state['response']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auditron_venv",
   "language": "python",
   "name": "auditron_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
